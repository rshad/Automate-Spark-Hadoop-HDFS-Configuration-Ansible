# first of all, we create hadoop user, so this user will be the one used to configure Hadoopa and HDFS
#- name: creating hadoop user
#  user:
#    name: hadoop
#    password: "{{ hadoop }}"
#    state: present
#    shell: /bin/bash    
#    system: no
#    createhome: yes
#    home: /home/hadoop

# -------------------------------- Installing Hadoop ------------------------------------------ #

# Note:  We do not need to donwload the java package from internet, we have our own already downloaded.
#- name: Extracting Hadoop package
#
#  unarchive:
#    src: "{{ hadoop_package_file }}"
#    dest: "{{ download_folder }}"
#  when: inventory_hostname in groups['masters']
#
#- name: "Changing the hadoop folder name from {{ hadoop_folder_path_name_old }} to {{ download_folder }}/{{ hadoop_folder_name_new }}"
#
#  command: "mv {{ hadoop_folder_path_name_old }}  {{ download_folder }}/{{ hadoop_folder_name_new }}"
#  when: inventory_hostname in groups['masters']
#
#- name: Copying hadoop/ folder to the slaves hosts ...
#  synchronize:
#    src: "{{ download_folder }}/{{ hadoop_folder_name_new }}"
#    dest: "{{ download_folder }}/"
#  when: inventory_hostname in groups['slaves']  

# Setting the required path for hadoop ...
#- name: Writing Hadoop Path's into /home/hadoop/.bashrc ...
#  lineinfile:
#    path: /home/hadoop/.bashrc
#    line: "{{ item }}"

#  become: yes
#  become_user: hadoop

#  with_items: 
#    - export HADOOP_PREFIX=/opt/hadoop
#    - export HADOOP_HOME=$HADOOP_PREFIX
#    - export HADOOP_COMMON_HOME=$HADOOP_PREFIX
#    - export HADOOP_CONF_DIR=$HADOOP_PREFIX/etc/hadoop
#    - export HADOOP_HDFS_HOME=$HADOOP_PREFIX
#    - export HADOOP_MAPRED_HOME=$HADOOP_PREFIX
#    - export HADOOP_YARN_HOME=$HADOOP_PREFIX
#    - export PATH=$PATH:$HADOOP_PREFIX/sbin:$HADOOP_PREFIX/bin

# Applying the changes
#- name: applying the changes ...
#  shell: source ~/.bashrc
#  become: yes
#  become_user: hadoop

# -------------------------------- Configuring Hadoop ------------------------------------------ #

#- name: Installing lxml python library ...
#  command: pip install lxml

#- name: Modifying the file /opt/hadoop/etc/hadoop/core-site.xml  ...
#  xml:
#    path: /opt/hadoop/etc/hadoop/core-site.xml
#    xpath: /configuration/property        
#    value:

#- name: Modifying the file /opt/hadoop/etc/hadoop/core-site.xml  ...
#  xml:
#    path: /opt/hadoop/etc/hadoop/core-site.xml
#    xpath: /configuration/property/name
#    value: fs.defaultFS

#- name: Modifying the file /opt/hadoop/etc/hadoop/core-site.xml  ...
#  xml:
#    path: /opt/hadoop/etc/hadoop/core-site.xml
#    xpath: /configuration/property/value
#    value: hdfs://central-machine:9000/

#- name: Creating the required folders for Hadoop ...
#  command: "{{ item }}"
#  args: 
#    warn: False  
#  with_items:
#    - chown hadoop /opt/hadoop/ -R 
#    - chgrp hadoop /opt/hadoop/ -R
#    - mkdir /home/hadoop/datanode
#    - chown hadoop /home/hadoop/datanode/
#    - chgrp hadoop /home/hadoop/datanode/

#- name: changing the name of the old template of /opt/hadoop/etc/hadoop/hdfs-site.xml ... to .original
#  command: cp /opt/hadoop/etc/hadoop/hdfs-site.xml /opt/hadoop/etc/hadoop/hdfs-site.xml.original
#
#- name: adding  /opt/hadoop/etc/hadoop/hdfs-site.xml with new content ... 
#  template:
#    src: ../templates/template-hdfs-site-xml.j2
#    dest: /opt/hadoop/etc/hadoop/hdfs-site.xml
#
#- name: Creating namenode folder in the master ...
#  command: "{{ item }}"
#
#  args:
#    warn: False
# 
#  with_items:
#     - mkdir /home/hadoop/namenode
#     - chown hadoop /home/hadoop/namenode/
#     - chgrp hadoop /home/hadoop/namenode/

#  when: inventory_hostname in groups['masters']


#- name: Creating the file /opt/hadoop/etc/hadoop/mapred-site.xml, using template ..
#
#  template:
#    src: ../templates/template-mapred-site-xml.j2
#    dest: /opt/hadoop/etc/hadoop/mapred-site.xml  
#
#  when: inventory_hostname in groups['masters']


#- name: Adding the names of the hosts of the slaves including the master ...
#  lineinfile:
#    path: /opt/hadoop/etc/hadoop/slaves
#    line: "{{ item }}" 
#
#  become: yes
#  become_user: hadoop
#
#  with_items:
#    - central-machine
#    - node-ninion-1
#    - node-minion-2
#
#  when: inventory_hostname in groups['masters']
#
#- name: Modifying the file /etc/sysctl.conf  ...
#  lineinfile:
#    path: /etc/sysctl.conf
#    line: "{{ item }}"
#
#  with_items:
#    - net.ipv6.conf.all.disable_ipv6 = 1
#    - net.ipv6.conf.default.disable_ipv6 = 1

- name: Clearing HDFS logs, temporal files and old data on masters ...
  command: "rm -rf {{ item }}"
  with_items:
    - /opt/hadoop/logs/*
    - ~/datanode/*
    - ~/namenode/*
    - /tmp/hadoop-hadoop/*
  become_user: hadoop
  when: inventory_hostname in groups['masters']


- name: Clearing HDFS logs, temporal files and old data on slaves ...
  command: "rm -rf {{ item }}"
  with_items:
    - /opt/hadoop/logs/*
    - ~/datanode/*
  become_user: hadoop
  when: inventory_hostname in groups['slavess']
